{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nama : Najla Dhia Rusydi\n",
    "\n",
    "NIM  : 164221043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>Natural Language Processing</strong><br />\n",
    "<strong><font color=\"blue\">Semester Gasal T.A. 2024/2025</font></strong><br />\n",
    "</center>\n",
    "\n",
    "<strong>Outline pertemuan minggu ke-4</strong><br />\n",
    "<li> Tokenisasi </li>\n",
    "<li> Casefolding </li>\n",
    "<li> Stemming dan Lemmatization</li>\n",
    "<li> Part of Speech (POS) tagging </li>\n",
    "<li> Stopword removal </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah text untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"figures\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Tokenisasi-dengan-modul-NLTK\">Tokenisasi dengan modul NLTK</h2>\n",
    "<p>NLTK dapat melakukan tokenisasi pada level kata dan pada level kalimat</p>\n",
    "\n",
    "<p><strong>Kelebihan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Well established dengan dukungan bahasa yang beragam</li>\n",
    "\t<li>Salah satu modul NLP dengan fungsi terlengkap, termasuk WordNet</li>\n",
    "\t<li>Free dan mendapat banyak dukungan akademis.</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Murni Python: relatif lebih lambat</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://www.nltk.org/\" target=\"_blank\">https://www.nltk.org/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenisasi kata\n",
    "from nltk import word_tokenize\n",
    "\n",
    "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "word_token = word_tokenize(S)\n",
    "\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'Mr.', 'Man.', 'He', 'smiled!!', 'This,', 'i.e.', 'that,', 'is', 'it.']\n"
     ]
    }
   ],
   "source": [
    "# Mengapa tidak menggunakan fungsi split dari python?? apa bedanya?\n",
    "\n",
    "word_split = S.split()\n",
    "print(word_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "karna dengan menggunakan split maka pemisahannya berdasanrkan spasi/tanda spesifik lainnya jika di tuliskan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# tokenisasi kalimat\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sentence_token = sent_tokenize(S)\n",
    "print(sentence_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 1:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah tanda baca seperti &quot;?&quot; atau &quot;!&quot; akan memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda &quot;carriage return&quot;/enter/ganti baris memisahkan kalimat?</li>\n",
    "\t<li>Apakah &quot;;&quot; memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda dash &quot;-&quot; memisahkan kata? Dalam bahasa Indonesia/Inggris?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi NLTK.<br />\n",
    "<strong>Catatan</strong>: pindah baris di Python string bisa dilakukan dengan menggunakan symbol &quot;\\n&quot;<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisasi Kalimat:\n",
      "['Halo, apa kabar?', 'Sudah lama tidak bertemu!', 'Ayo, kita ngopi.', 'Kalimat selanjutnya.', 'Aku suka dia; dia lebih suka dengan teh.', 'Saya ingin menjadi ikan Hiu.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Halo, apa kabar? Sudah lama tidak bertemu! Ayo, kita ngopi.\\nKalimat selanjutnya. Aku suka dia; dia lebih suka dengan teh. Saya ingin menjadi ikan Hiu.\"\n",
    "\n",
    "# Tokenisasi kalimat\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Tokenisasi Kalimat:\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JAWAB**\n",
    "\n",
    "Dengan percobaan diatas dapat diketahui :\n",
    "- Tanda baca '?' atau '!' dapat memisahkan kalimat ('Halo, apa kabar?', 'Sudah lama tidak bertemu!', 'Ayo, kita ngopi.')\n",
    "- Enter atau memindah baris juga dapat memisahkan kalimat ( 'Ayo, kita ngopi.', 'Kalimat selanjutnya.')\n",
    "- Tanda ';' tidaklah memisahkan kalimat ('Aku suka dia; dia lebih suka dengan teh.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenisasi Kata:\n",
      "['Hitam-Putih', 'itu', 'bahasa', 'inggrisnya', 'Black-White']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hitam-Putih itu bahasa inggrisnya Black-White \"\n",
    "# Tokenisasi kata\n",
    "words = word_tokenize(text)\n",
    "print(\"\\nTokenisasi Kata:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JAWAB**\n",
    "\n",
    "Denagn melakukan percobaan diatas, dapat diketahui bahwah '-' tidak memisahkan kata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi dengan modul Spacy\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Di claim lebih cepat (C-based)</li>\n",
    "\t<li>License termasuk untuk komersil</li>\n",
    "\t<li>Dukungan bahasa yang lebih banyak dari NLTK (termasuk bahasa Indonesia*)</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Fungsi yang lebih terbatas (dibandingkan NLTK).</li>\n",
    "\t<li>Karena berbasis compiler, sehingga instalasi cukup menantang.</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://spacy.io/\" target=\"_blank\">https://spacy.io/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi kata menggunakan Spacy\n",
    "import spacy\n",
    "\n",
    "# Loading language model\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "tokens = spacy_en(S)\n",
    "print( [token.text for token in tokens] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e. that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi kalimat dengan Spacy\n",
    "sentence_tokens = spacy_en(S).sents\n",
    "print([str(sent) for sent in sentence_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 2:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah hasil tokenisasi Spacy = NLTK? Mengapa?</li>\n",
    "\t<li>Lakukan latihan seperti yang dilakukan sebelumnya dengan modul NLTK, apakah hasilnya sama dengan Spacy?</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi Spacy juga berbeda dengan NLTK.<br />\n",
    "<strong>Catatan</strong>: Contoh sederhana ini menekankan perbedaan ilmu linguistik dan computational linguistic.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisasi Kalimat dengan NLTK:\n",
      "['Halo, apa kabar?', 'Sudah lama tidak bertemu!', 'Ayo, kita ngopi.', 'Kalimat selanjutnya.', 'Aku suka dia; dia lebih suka dengan teh.', 'Saya ingin menjadi ikan-Hiu.']\n",
      "\n",
      "Tokenisasi Kata dengan NLTK:\n",
      "['Halo', ',', 'apa', 'kabar', '?', 'Sudah', 'lama', 'tidak', 'bertemu', '!', 'Ayo', ',', 'kita', 'ngopi', '.', 'Kalimat', 'selanjutnya', '.', 'Aku', 'suka', 'dia', ';', 'dia', 'lebih', 'suka', 'dengan', 'teh', '.', 'Saya', 'ingin', 'menjadi', 'ikan-Hiu', '.']\n",
      "\n",
      "Tokenisasi Kalimat dengan SpaCy:\n",
      "['Halo, apa kabar?', 'Sudah lama tidak bertemu!', 'Ayo, kita ngopi.\\n', 'Kalimat selanjutnya.', 'Aku suka dia; dia lebih suka dengan teh.', 'Saya ingin menjadi ikan-Hiu.']\n",
      "\n",
      "Tokenisasi Kata dengan SpaCy:\n",
      "['Halo', ',', 'apa', 'kabar', '?', 'Sudah', 'lama', 'tidak', 'bertemu', '!', 'Ayo', ',', 'kita', 'ngopi', '.', '\\n', 'Kalimat', 'selanjutnya', '.', 'Aku', 'suka', 'dia', ';', 'dia', 'lebih', 'suka', 'dengan', 'teh', '.', 'Saya', 'ingin', 'menjadi', 'ikan', '-', 'Hiu', '.']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan 2 pada cell berikut ini\n",
    "\n",
    "# Model SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Halo, apa kabar? Sudah lama tidak bertemu! Ayo, kita ngopi.\\nKalimat selanjutnya. Aku suka dia; dia lebih suka dengan teh. Saya ingin menjadi ikan-Hiu.\"\n",
    "\n",
    "# Tokenisasi dengan NLTK\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "nltk_words = word_tokenize(text)\n",
    "\n",
    "print(\"Tokenisasi Kalimat dengan NLTK:\")\n",
    "print(nltk_sentences)\n",
    "\n",
    "print(\"\\nTokenisasi Kata dengan NLTK:\")\n",
    "print(nltk_words)\n",
    "\n",
    "# Tokenisasi dengan SpaCy\n",
    "doc = nlp(text)\n",
    "spacy_sentences = [sent.text for sent in doc.sents]\n",
    "spacy_words = [token.text for token in doc]\n",
    "\n",
    "print(\"\\nTokenisasi Kalimat dengan SpaCy:\")\n",
    "print(spacy_sentences)\n",
    "\n",
    "print(\"\\nTokenisasi Kata dengan SpaCy:\")\n",
    "print(spacy_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JAWAB**\n",
    "\n",
    "Berdasarkan percobaan diatas, didapatkan bahwa :\n",
    "- Untuk tokenisasi kalimat, NLTK memisahkan kalimat setelah enter '\\n', sedangkan Spacy tidak.\n",
    "- Untuk tokenisasi kata, NLTK tidak memisahkan kata dengan tanda '-', sedangkan pada Spacy tanda '-'dapat memisahkan kata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi dengan TextBlob\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Sederhana &amp; mudah untuk digunakan/pelajari.</li>\n",
    "\t<li>Textblob objects punya behaviour/properties yang sama dengan string di Python.</li>\n",
    "\t<li>TextBlob dibangun dari kombinasi modul NLTK dan (Clips) Pattern</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Tidak secepat Spacy dan NLTK</li>\n",
    "\t<li>Language Model terbatas: English, German, French</li>\n",
    "</ol>\n",
    "\n",
    "<p>*Blob : Binary large Object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', 'Man', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi dengan TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "sentence_tokens = TextBlob(T).sentences\n",
    "\n",
    "# Tokenisasi kata\n",
    "print(TextBlob(T).words)\n",
    "\n",
    "# Tokenisasi kalimat\n",
    "print([str(sent) for sent in sentence_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 3:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ada yang berbeda dari hasilnya?&nbsp;Apakah lebih baik seperti ini?</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Tips</strong>: TextBlob biasa digunakan untuk prototyping pada data yang tidak terlalu besar.<br />\n",
    "<strong>Catatan</strong>: Hati-hati tipe data Blob tidak biasa (objek).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenisasi dengan TextBlob:\n",
      "['Hello', 'Mr', 'Man', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n",
      "\n",
      "Tokenisasi dengan SpaCy:\n",
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '.']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e. that, is it.']\n",
      "\n",
      "Tokenisasi dengan NLTK:\n",
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan 3 pada cell berikut ini\n",
    "print(\"\\nTokenisasi dengan TextBlob:\")\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "sentence_tokens = TextBlob(T).sentences\n",
    "print(TextBlob(T).words)\n",
    "print([str(sent) for sent in sentence_tokens])\n",
    "\n",
    "print(\"\\nTokenisasi dengan SpaCy:\")\n",
    "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "tokens = spacy_en(S)\n",
    "print( [token.text for token in tokens] )\n",
    "sentence_tokens = spacy_en(S).sents\n",
    "print([str(sent) for sent in sentence_tokens])\n",
    "\n",
    "print(\"\\nTokenisasi dengan NLTK:\")\n",
    "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "word_token = word_tokenize(S)\n",
    "print(word_token)\n",
    "sentence_token = sent_tokenize(S)\n",
    "print(sentence_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JAWAB**\n",
    "\n",
    "Hasil yang didapat menampilkan perbedaan, dengan menggunakan TextBlob tandabaca seperti `','/'.'/'!'` pada pesmisahan kata dihilangkan.\n",
    "\n",
    "Lebih baik mana?\n",
    "1. Kelebihan TextBlob:\n",
    "    - Mudah digunakan, TextBlob lebih sederhana untuk bekerja dengan data kecil hingga menengah.\n",
    "    - Cepat dan Mudah, bagus untuk tugas-tugas dasar dan eksperimen cepat.\n",
    "2. Kekurangan TextBlob:\n",
    "    - Kurang Akurat untuk text yang Kompleks, tidak seakurat SpaCy dalam memproses text yang lebih kompleks atau yang memerlukan pemahaman kontexttual yang lebih dalam.\n",
    "    - Terbatas untuk Prototyping, TextBlob lebih cocok untuk prototyping dan analisis sederhana, bukan untuk aplikasi produksi besar.\n",
    "\n",
    "jadi jawabnnya tergantung dengan keperluannya, jika digunakan hanay untuk prototype atau nalasis sesder hana TextBlob baik digunakan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi: language dependent dan environment dependent\n",
    "\n",
    "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"figures/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bagaimana untuk data yang punya karakteristik tertentu seperti Twitter?__\n",
    "__Apakah bisa menggunakan modul tokenisasi yang telah tersedia?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'so', 'happpy', ',', 'supeeer', 'happpy', ':)', '#imsohappy', '#happy']\n"
     ]
    }
   ],
   "source": [
    "# Contoh Tokenizer untuk twitter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "Tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = \"@stki I am so happpppppppy, supeeeer happpy :) #imsohappy #happy\"\n",
    "print(Tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@stki I am so happy\n"
     ]
    }
   ],
   "source": [
    "import itertools \n",
    "\n",
    "tweet = \"@stki I am so happpppppppppy\"\n",
    "\n",
    "# Menghilangkan double karakter\n",
    "tweet_clear = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "print(tweet_clear)\n",
    "\n",
    "# NOTES: untuk beberapa data spesifik seperti data bioinformatics, cryptography,\n",
    "# twitter, dst dibutuhkan tokenizer custom untuk dapat memenuhi kebutuha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi (NLP) Bahasa Indonesia:\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "\n",
    "# load language model bahasa Indonesia\n",
    "spacy_id = Indonesian()\n",
    "\n",
    "S = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
    "word_token = spacy_id(S)\n",
    "print([token.text for token in word_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Jika menggunakan Language model English:\n",
    "word_token_en = spacy_en(S)\n",
    "print([token.text for token in word_token_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 4:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah ada perbedaan apabila menggunakan language model yang berbeda?</li>\n",
    "    <li>Bagaimana jika melakukan tokenisasi Bahasa Indonesia dengan NLTK? Apakah hasilnya sama dengan Spacy?\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan bahasa Indonesia :\n",
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n",
      "\n",
      "Menggunakan bahasa  Inggris :\n",
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan Latihan 4 pada cell berikut ini\n",
    "print('Menggunakan bahasa Indonesia :')\n",
    "print([token.text for token in word_token])\n",
    "\n",
    "print(\"\\nMenggunakan bahasa  Inggris :\")\n",
    "print([token.text for token in word_token_en])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jawab**\n",
    "\n",
    "Terdapat perbedaan diantara tokenisasi menggunakan bahasa indonesia dan bahasa inggris, terlihat pada kata yang memiliki tanda hubung '-' seperti 'kupu-kupu' dan 'oleh-oleh'. pada bahasa indonesia, tentusaja tokenisasi memahami bahwa 'kupu-kupu' merupakan 1 kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisasi Kata dengan NLTK:\n",
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n",
      "\n",
      "Tokenisasi Kata dengan SpaCy:\n",
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenisasi kata dengan NLTK\n",
    "nltk_tokens = word_tokenize(S)\n",
    "print(\"Tokenisasi Kata dengan NLTK:\")\n",
    "print(nltk_tokens)\n",
    "\n",
    "# Tokenisasi bahasa indonesia sengan SpaCy\n",
    "print(\"\\nTokenisasi Kata dengan SpaCy:\")\n",
    "print([token.text for token in word_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jawab**\n",
    "\n",
    "Dapat dilihat bahwa tidak ada perbedaan dari hasil diantara keduanya, walaupun pada NLTK tidak memiliki model khusus untuk melakukan tokenisasi dalam bahasa indonesia, namun NLTK tidak memiliki perbedaan dnegan SpaCy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casefolding\n",
    "\n",
    "<p> Casefolding dilakukan untuk merubah karakter ke dalam huruf besar (uppercase) atau huruf kecil (lowercase) </p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "    <li>Casefolding dapat dilakukan dengan efisien tanpa melalui proses tokenisasi</li>\n",
    "\t<li>Namun, bergantung pada analisa text yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there!, i am a student. nice to meet you :)\n",
      "HI THERE!, I AM A STUDENT. NICE TO MEET YOU :)\n"
     ]
    }
   ],
   "source": [
    "# Contoh casefolding\n",
    "S = \"Hi there!, I am a student. Nice to meet you :)\"\n",
    "print(S.lower())\n",
    "print(S.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 5:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Temukan minimal 2 pengecualian dimana huruf besar dan kecil mempengaruhi makna dalam pemrosesan text</li>\n",
    "    <li>Mengapa casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi?</li>\n",
    "    <li>Berikan contoh pengaruh dari urutan proses dalam preprocessing yang berpengaruh terhadap hasil preprocessing </li>\n",
    "        \n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jawab**\n",
    "\n",
    "- Huruf kapital dan huruf kecil pada suatu kata bisa memberikan makna yang berbeda, seperti contohnya sebuah brand 'Apple' dengan 'apel' buah, selain itu Huruf besar juga bisa menandakan suatu kata itu penting seperti nama kota 'Jakarta', huruf kapital J menandakan bahwa kata tersebut memiliki makna yang penting dalam suatu text.\n",
    "- Casefolding merupakan proses mengubah semua huruf menjadi huruf kecil, operasi ini tidak bergantung pada struktur atau makna text. Proses ini hanya mengubah semua huruf kapital menjadi huruf kecil tanpa perlu memperhatikan kontext kata atau kalimat. Oleh karena itu, casefolding dapat dilakukan sebelum tokenisasi karena tidak perlu mengetahui batas kata atau kalimat untuk mengubah huruf besar menjadi huruf kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casefolding dulu, lalu tokenisasi: ['ikan', 'hiu', 'makan', 'tomat', 'dan', 'tomat', 'segar', '.']\n",
      "Tokenisasi dulu, lalu casefolding: ['ikan', 'hiu', 'makan', 'tomat', 'dan', 'tomat', 'segar', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Ikan Hiu Makan Tomat dan Tomat Segar.\"\n",
    "\n",
    "# Casefolding sebelum tokenisasi\n",
    "casefolded_text = text.lower()\n",
    "tokens_after_casefolding = word_tokenize(casefolded_text)\n",
    "print(\"Casefolding dulu, lalu tokenisasi:\", tokens_after_casefolding)\n",
    "\n",
    "# Tokenisasi sebelum casefolding\n",
    "tokens = word_tokenize(text)\n",
    "casefolded_tokens = [token.lower() for token in tokens]\n",
    "print(\"Tokenisasi dulu, lalu casefolding:\", casefolded_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jawab**\n",
    "\n",
    "pecobaan pertama casefolding dulu, baru tokenisasi. sedangkan yang ke dua tokenisasi dulu, baru casefolding. dilihat dari percobaan di atas dapat dilihat bahwa kedua percobaan tersebut tidak terdapat perbedaan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming dan Lemmatization\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan kontext kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung kontext.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stemming tidak perlu \"benar\", hanya perlu konsisten__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  presumably I would like to MultiPly my provision, saying tHat without crYing\n",
      "Lancaster :  presum i would lik to multiply my provision, say that without cry\n",
      "Porter :  presum i would like to multipli my provision, say that without cri\n",
      "SnowBall :  presum i would like to multipli my provision, say that without cri\n"
     ]
    }
   ],
   "source": [
    "# Contoh Stemming di NLTK\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "S = 'presumably I would like to MultiPly my provision, saying tHat without crYing'\n",
    "print('Sentence: ',S)\n",
    "\n",
    "stemmer_list = [LancasterStemmer, PorterStemmer, SnowballStemmer] \n",
    "names = ['Lancaster', 'Porter', 'SnowBall']\n",
    "for stemmer_name,stem in zip(names,stemmer_list):\n",
    "    if stemmer_name == 'SnowBall':\n",
    "        st = stem('english')\n",
    "    else:\n",
    "        st = stem()\n",
    "    print(stemmer_name,': ',' '.join(st.stem(s) for s in S.split()))\n",
    "# perhatikan, kita tidak melakukan case normalization (lowercase) \n",
    "# Hasil stemming bisa tidak bermakna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  apples and Oranges are similar. boots and hippos aren't, don't you think?\n",
      "Lemmatize:  apple and Oranges are similar. boot and hippo aren't, don't you think?\n"
     ]
    }
   ],
   "source": [
    "# Contoh Lemmatizer di NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "S = \"apples and Oranges are similar. boots and hippos aren't, don't you think?\"\n",
    "print('Sentence: ', S)\n",
    "print('Lemmatize: ',' '.join(lemmatizer.lemmatize(s) for s in S.split()))\n",
    "# Lemma case sensitive. Dengan kata lain string harus diubah ke dalam bentuk huruf kecil (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer menggunakan informasi pos. \"pos\" (part-of-speech) akan dibahas di segmen berikutnya\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adjective\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"v\")) # verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize:  go\n",
      "Lemmatize:  went\n",
      "Lemmatize:  went\n",
      "Lemmatize:  went\n"
     ]
    }
   ],
   "source": [
    "# Contoh TextBlob Stemming & Lemmatizer\n",
    "from textblob import Word\n",
    "# Stemming\n",
    "#print(\"Stem: \", Word('running').stem()) # menggunakan NLTK Porter stemmer\n",
    "\n",
    "# Lemmatizer\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('v'))\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('n'))\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('a'))\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('r'))\n",
    "\n",
    "\n",
    "# default Noun, plural akan menjadi singular dari akar katanya\n",
    "# Juga case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'playing', 'better', 'than', 'me']\n",
      "['you', 'are', 'playing', 'better', 'than', 'me']\n",
      "['you', 'be', 'play', 'better', 'than', 'me']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob,Word\n",
    "sentence=\"you are playing better than me\"\n",
    "w=sentence.split(\" \")\n",
    "print(w)\n",
    "print([Word(word).lemmatize() for word in w])\n",
    "print([Word(word).lemmatize(\"v\") for word in w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be sure apple and orange be similar . boot and hippos be not , do not you think ?\n"
     ]
    }
   ],
   "source": [
    "# Spacy Lemmatizer English\n",
    "sent = \"I am sure Apples and oranges are similar. Boots and hippos aren't, don't you think?\"\n",
    "sent_token = spacy_en(sent)\n",
    "print( ' '.join( s.lemma_ for s in sent_token ) )\n",
    "# HATI-HATI dengan lemma \"I\" dan \"you\" di Spacy!!!..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n"
     ]
    }
   ],
   "source": [
    "# Spacy Lemmatizer Indonesia\n",
    "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Jogjakarta\"\n",
    "idn = spacy_id(I)\n",
    "print( ' '.join( k.lemma_ for k in idn ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '']\n"
     ]
    }
   ],
   "source": [
    "# Perhatikan output berikut (hati-hati inkonsistensi)\n",
    "print([k.lemma_ for k in spacy_id(\"Perayaan Bepergian\")])\n",
    "\n",
    "# bagaimana dengan Spacy stemmer??\n",
    "# Spacy belum support stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raya itu bareng dengan saat kita pergi ke makassar\n",
      "raya pergi suara\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Makassar\"\n",
    "print(stemmer.stem(I))\n",
    "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
    "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
    "# Apa sajakah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Tips:\">Tips:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Secara umum &#39;biasanya&#39; di Text Mining yang kita butuhkan hanyalah <strong><font color=\"blue\">Lemma</font></strong>.</li>\n",
    "\t<li>&quot;Kecuali&quot; di aplikasi IR, spelling correction, variasi kata, clustering, atau terkadang klasifikasi. Pada aplikasi-aplikasi tersebut stemming terkadang lebih diinginkan.</li>\n",
    "\t<li>Stemming jauh lebih cepat, tapi tidak selalu tersedia di modul NLP.</li>\n",
    "\t<li>Beberapa algoritma tertentu membutuhkan tanda &quot;.&quot; dan &quot;,&quot; : contohnya untuk document summarization di textRank.</li>\n",
    "\t<li>&quot;_&quot; juga biasa digunakan untuk menyatakan frase kata di representasi n-grams (misal &quot;buah_tangan&quot;).</li>\n",
    "\t<li>Stemming juga digunakan pada Word Sense Disambiguation (WSD)</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"Diskusi:\">Diskusi:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menghemat storage database, apakah sebaiknya kita menyimpan saja hasil preprocessed texts/documents?</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jawab**\n",
    "1. Keuntungan Menyimpan Hasil Preprocessed:\n",
    "\n",
    "    - Hemat Ruang Penyimpanan: Menyimpan hasil preprocessed text (hasil dari proses tokenized, lemmatized, atau stemmed text) dapat menghemat ruang penyimpanan dibandingkan menyimpan text asli yang mungkin lebih panjang dan memerlukan lebih banyak ruang.\n",
    "    - Percepatan Proses: Menggunakan hasil preprocessed bisa mempercepat proses analisis selanjutnya, karena tidak perlu memproses text dari awal setiap kali.\n",
    "\n",
    "2. Kerugian Menyimpan Hasil Preprocessed:\n",
    "    - Kehilangan Data Asli: Setelah preprocessing, informasi yang mungkin penting untuk analisis di masa depan (seperti struktur kalimat asli atau kontext yang lebih luas) bisa hilang.\n",
    "    - Reproses untuk Kebutuhan Baru: Jika ada perubahan dalam metode preprocessing atau jika perlu melakukan analisis baru dengan metode yang berbeda, Anda mungkin perlu melakukan preprocessing ulang dari text asli.\n",
    "\n",
    "Rekomendasi:\n",
    "\n",
    "Keputusan untuk menyimpan hasil preprocessed sebaiknya didasarkan pada kebutuhan spesifik proyek dan ketersediaan sumber daya. Dalam kasus di mana penyimpanan dan kecepatan adalah prioritas utama, menyimpan hasil preprocessed bisa menjadi pilihan yang baik. Namun, jika fleksibilitas dan kemampuan untuk melakukan analisis tambahan di masa depan adalah prioritas, menyimpan text asli bisa lebih disarankan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) tag\n",
    "\n",
    "<p> POS tagging merupakan proses pemberian tanda berupa kelas kata pada setiap kata yang terdapat di dalam corpus.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"figures/2_parts-of-speech-chart.jpg\" style=\"height:400px; width:404px\" /></p>\n",
    "<p>[<a href=\"https://www.paperrater.com/page/parts-of-speech\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# Contoh POS tags dengan NLTK (bahasa Inggris)\n",
    "from nltk import pos_tag\n",
    "S = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
    "\n",
    "tokens = word_tokenize(S)\n",
    "print(pos_tag(tokens))\n",
    "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daftar POS tag NLTK:\n",
    "\n",
    "<img alt=\"\" src=\"figures/2_post_tags_NLTK.png\" style=\"height:400px; width:516px\" /></h3>\n",
    "\n",
    "<p>[<a href=\"http://gitqwerty777.github.io/natural-language-processing/\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('currently', 'RB'),\n",
       " ('learning', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('English', 'NNP'),\n",
       " ('but', 'CC'),\n",
       " ('if', 'IN'),\n",
       " ('possible', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('know', 'VB'),\n",
       " ('NLP', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Indonesian', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('too', 'RB')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh POS tag dengan TextBlob pada bahasa Inggris\n",
    "blob = TextBlob(S)\n",
    "blob.tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello UH, , ,, Mr. NNP, Man NNP, . ., He PRP, smiled VBD, ! ., ! ., This DT, , ,, i.e. FW, that IN, , ,, is VBZ, it PRP, . ., "
     ]
    }
   ],
   "source": [
    "# Contoh POS tag dengan Spacy pada bahasa Inggris\n",
    "tokens = spacy_en(T)\n",
    "for token in tokens:\n",
    "    print(token,token.tag_, end =', ')\n",
    "    \n",
    "# Spacy belum support POS tag untuk bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverb'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Untuk mengetahui arti dari POS tag pada Spacy dapat menggunakan perintah \"explain\"\n",
    "spacy.explain('RB')\n",
    "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 6:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Kapan harus melakukan POS tagging pada tahap preprocessing?</li>\n",
    "    <li>Buatlah contoh hasi dari POS tag dengan hanya mengambil kata yang memiliki tag NOUN (*), dan berikan contoh kasus penggunaannya?</li>\n",
    "    <li>Buatlah contoh hasil dari POS tag yang telah ditambahkan pada setiap kata dalam suatu kalimat dengan menggunakan NLTK (**)</li>\n",
    "        \n",
    "</ul>\n",
    "\n",
    "<p>(*) <strong>Input</strong>: \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"</p>\n",
    "<p>(**)</p> \n",
    "<p> <strong>Input</strong>: \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"</p>\n",
    "<p> <strong>Expected output</strong>: \"I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jawaban**\n",
    "\n",
    "- POS tagging dilakukan setelah melakukan tahap tokenisasi dan sebelum tahap-tahap yang membutuhkan analisis gramatikal atau struktur kalimat \n",
    "- Biasanya, POS tagging diperlukan katika kita perlu memahami fungsi suatu kata yang ada dalam kalimat, contohnya :\n",
    "    - Penyaringan kata berdasarkan tipe kata (kata kerja/kata benda).\n",
    "    - Lemmatization bergantung pada POS untuk menentukan bentuk kata dasar yang tepat.\n",
    "    - Text generation dan machine translation, di sini pemahaman dari peran pada setiap kata itu penting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiger', 'Panthera', 'tigris', 'cat', 'species', 'member', 'genus', 'Panthera', 'stripes', 'fur', 'underside', 'predator', 'ungulates', 'deer', 'boar']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan 6 pada cell berikut ini\n",
    "text = \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"\n",
    "\n",
    "# Tokenisasi dan POS tagging\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Filter kata dengan tag NOUN (NN, NNS, NNP, NNPS)\n",
    "nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "\n",
    "print(nouns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan**\n",
    "\n",
    "POST tags dapat digunakan untuk menyaring Noun/kata benda dari text, dapat bermanfaat untuk document summarization atau information extraction. Di mana kita hanya ingin fokus pada entitas atau objek tertentu seperti contoh yang telah dikerjakan kita mengambil nama hewan serta spesies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB ._.\n"
     ]
    }
   ],
   "source": [
    "text = \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too.\"\n",
    "\n",
    "# Tokenisasi dan POS tagging\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Gabungkan kata dengan tag POS\n",
    "tagged_sentence = ' '.join([f\"{word}_{pos}\" for word, pos in pos_tags])\n",
    "\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan**\n",
    "\n",
    "Menambahkan POS tag pada setiap kata dalam kalimat bermanfaat untuk memahami tata bahasa dan mengenali pola, seperti dalam pembuatan text otomatis, analisis kalimat, dan analisis sentimen. Dengan tag POS, kita bisa lebih mudah memahami susunan kalimat dan melakukan pengolahan text yang lebih akurat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal\n",
    "\n",
    "<p> Stopword removal merupakan salah satu cara untuk melakukan normalisasi pada level kata </p>\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n"
     ]
    }
   ],
   "source": [
    "# Contoh stopword dari NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stw_en = stopwords.words('english')\n",
    "print(nltk_stw_en[:10])\n",
    "\n",
    "nltk_stw_id = stopwords.words('indonesian')\n",
    "print(nltk_stw_id[:10])\n",
    "\n",
    "# Lsit stopword dapat ditambahkan dan dikurangi sesuai dengan kebutuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua']\n"
     ]
    }
   ],
   "source": [
    "#contoh stopword dari Sastrawi pada bahasa Indonesia\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "sastrawi_stw_id = factory.get_stop_words()\n",
    "print(sastrawi_stw_id[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips:\n",
    "# selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
    "nltk_stw_en = set(nltk_stw_en)\n",
    "nltk_stw_id = set(nltk_stw_id)\n",
    "sastrawi_stw_id = set(sastrawi_stw_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 7:</font></h3>\n",
    "\n",
    "<p> Lakukan stopword removal pada contoh paragraf berikut ini: </p>\n",
    "<p> \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\" </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks setelah stopword removal: Siti Nurbaya novel Indonesia ditulis Marah Rusli . Novel diterbitkan Balai Pustaka , penerbit nasional negeri Hindia Belanda , 1922 .\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan 7 pada cell berikut ini\n",
    "\n",
    "text = \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\"\n",
    "\n",
    "# Tokenisasi text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Ambil daftar stopwords bahasa Indonesia (default NLTK tidak memiliki stopwords Indonesia, jadi pastikan mengunduh atau menggunakan daftar khusus)\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Fungsi stopword removal\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Hasil\n",
    "print(\"text setelah stopword removal:\", ' '.join(filtered_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan**\n",
    "\n",
    "Stopword removal merupakan sebuah proses untuk menghapus kata-kata yg dianggap umum dan tidak memberikan banyak informasi penting dalam analisis text. Kata-kata ini disebut stopwords. salah satu bentuk stopword adalah kata penghubung, berdasarkan percobaan yang telah kita lakukan dapat kita lihat bahwa kata `'adalah', 'yang', 'oleh', 'ini'` dihapus setelah melakukan Stopword Removal. Tujuan dari stopword removal adalah untuk menyederhanakan text dan mengurangi kebisingan (noise), sehingga fokus analisis dapat diarahkan pada kata-kata yang lebih bermakna seperti kata benda, kata kerja, atau kata sifat yang penting untuk pemrosesan lanjutan seperti klasifikasi text atau pencarian informasi."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
