{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSs2Eak3UfJE"
      },
      "source": [
        "Nama    : Najla Dhia Rusydi\n",
        "\n",
        "NIM     : 164221043"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASZULsW2UfJG"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# **NER**\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgMoUR_RUfJH"
      },
      "source": [
        "**Named Entity Recognition (NER)** : salah satu preprocessing dalam NLP yang bertujuan untuk mengidentifikasi dan mengkategorikan entitas penting dalam sebuah teks. Entitas yang diidentifikasi biasanya termasuk kategori seperti:\n",
        "\n",
        "- **Orang** (Person): Nama individu, misalnya, *Barack Obama*.\n",
        "- **Lokasi** (Location): Nama tempat atau wilayah, misalnya, *New York* atau *Indonesia*.\n",
        "- **Organisasi** (Organization): Nama organisasi, misalnya, *Google* atau *PBB*.\n",
        "- **Tanggal** (Date): Identifikasi tanggal atau periode waktu, misalnya, *2021* atau *kemarin*.\n",
        "\n",
        "NER sangat berguna dalam berbagai aplikasi seperti:\n",
        "- Ekstraksi informasi penting dari artikel berita.\n",
        "- Menyusun database otomatis dari teks.\n",
        "- Menganalisis konten media sosial atau dokumen bisnis.\n",
        "\n",
        "Jadi, NER membantu dalam memahami dan mengekstrak informasi spesifik dari teks secara otomatis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0OyUlAUfJH"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "### **Percobaan NER (Named Entity Recognition) dengan NLTK dan Spacy**\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLWjaaceUfJI"
      },
      "source": [
        "#### **NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06kLoSHUfJI",
        "outputId": "c3b19bba-f70a-4e27-b1a4-3c71d3d33983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  hi/NN\n",
            "  ,/,\n",
            "  my/PRP$\n",
            "  name/NN\n",
            "  is/VBZ\n",
            "  (PERSON Nala/NNP)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Indonesia/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"hi, my name is Nala, I was born in Indonesia.\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(named_entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URpJA7YUUfJK"
      },
      "source": [
        "**Penjelasan Kode:**\n",
        "1. **Tokenisasi**: Kalimat dipecah menjadi kata-kata terpisah menggunakan `word_tokenize()`.\n",
        "2. **POS Tagging**: Setiap kata diberi label dengan Part-of-Speech (POS) menggunakan `pos_tag()`, seperti kata benda, kata kerja, dan lain-lain.\n",
        "3. **NER (Named Entity Recognition)**: Dengan menggunakan `ne_chunk()`, kata-kata yang terkait dengan entitas (seperti nama orang, tempat, atau organisasi) akan dikenali dan ditampilkan dalam bentuk chunking tree.\n",
        "\n",
        "**Penjelasan Output:**\n",
        "\n",
        "1. **S**: Melambangkan sebagai kalimat (*sentence*).\n",
        "2. **hi/NN**: \"hi\" diberi tag sebagai **NN (noun)**, yang berarti kata ini dianggap sebagai kata benda.\n",
        "3. **,/,:**: Koma sebagai tanda baca.\n",
        "4. **my/PRP$**: \"my\" diberi tag sebagai **PRP$ (possessive pronoun)**, menunjukkan kata ganti kepemilikan.\n",
        "5. **name/NN**: \"name\" diberi tag sebagai **NN (noun)**, kata benda.\n",
        "6. **is/VBZ**: \"is\" diberi tag sebagai **VBZ (verb, third-person singular present)**, untuk kata kerja bentuk present orang ketiga tunggal.\n",
        "7. **(PERSON Nala/NNP)**: \"nala\" diberi tag sebagai * **NNP (proper noun)** yang merupakan nama orang, dan di identifikasi sebagai entitas **person**.\n",
        "8. **,/,:**: Koma sebagai tanda baca.\n",
        "9. **I/PRP**: \"i\" diberi tag sebagai **PRP (personal pronoun)** yaitu kata ganti subjek.\n",
        "10. **was/VBD**: \"was\" diberi tag sebagai **VBD (verb, past tense)**, kata kerja bentuk lampau.\n",
        "11. **born/VBN**: \"born\" diberi tag sebagai **VBN (past participle)**, sebagai bentuk participle kata kerja.\n",
        "12. **in/IN**: \"in\" diberi tag sebagai **IN (preposition)**, sebagai kata depan.\n",
        "13. **(GPE Indonesia/NNP)**: \"Indonesia\" diberi tag sebagai **NNP (proper noun)**,karena ini adalah nama tempat (negara). selain intu juga di identifikasikan kedalam entitas Geografi.\n",
        "14. **./.**: Tanda titik sebagai akhir kalimat.\n",
        "\n",
        "**Kesimpulan:**\n",
        "- Ada beberapa kesalahan dalam penandaan **POS**. Misalnya:\n",
        "    - \"hi\" seharusnya ditandai sebagai UH (interjection), bukan NN (noun).\n",
        "- **NER** berhasil mengenali **Nala** sebagai entitas orang (**PERSON**) dan **Indonesia** sebagai entitas geografis (**GPE**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9F0Bf9_UfJK"
      },
      "source": [
        "#### **Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdiD9TuzUfJK",
        "outputId": "f4f29fb9-03e7-41f6-bb98-1810e4bc5eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nala PERSON\n",
            "Indonesia GPE\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKFTUQD5UfJL"
      },
      "source": [
        "**Penjelasan Kode:**\n",
        "\n",
        "Berbeda dengan NLTK, Spacy tidak perlu melakukan tokenisasi da ps taging secara terpisah.SpaCy telah mengurus semua langkah-langkah tersebut secara otomatis di dalam pipeline NLP-nya.\n",
        "\n",
        "1. Tokenisasi: SpaCy akan memecah teks menjadi token-token (kata dan tanda baca) ketika memproses teks dengan `nlp()`.\n",
        "2. POS Tagging: Setelah tokenisasi, spaCy secara otomatis memberikan tag POS pada setiap token.\n",
        "3. Named Entity Recognition (NER): SpaCy juga melakukan NER dalam proses tersebut, dan dapat langsung mengakses entitas yang dikenali melalui `doc.ents`.\n",
        "\n",
        "**Penjelasan Output:**\n",
        "\n",
        "Sama seperti NLTK kata **Nala** di idntifikasi sebagai entitas **PERSON** dan kata **Indonesia** sebagai entitas **GPE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "#### **Kesimpula**\n",
        "\n",
        "</div>\n",
        "\n",
        "Dalam melakukan Named Entity Recognition (NER), NLTK dan spaCy memiliki pendekatan yang berbeda. Di NLTK, proses NER memerlukan beberapa langkah yang dilakukan secara terpisah. Seperti, harus melakukan tokenisasi dengan menggunakan `word_tokenize`. kemudian, setiap token perlu diberi label kategori kata (POS tagging) menggunakan `pos_tag`. Dan kemudaian baru, melakukan identifikasi entitas melalui `ne_chunk`, yang memproses token dan label POS untuk menemukan entitas seperti nama orang, tempat, atau organisasi.\n",
        "\n",
        "Sebaliknya, spaCy menyederhanakan proses ini dengan mengintegrasikan tokenisasi, POS tagging, dan NER dalam satu langkah pemrosesan. dengan hanya memuat model dengan bahasa yang sesuai menggunakan `spacy.load()`, lalu memproses teks dengan metode `nlp()`. Hasilnya, spaCy langsung memberikan entitas bernama yang ditemukan dalam teks, tanpa perlu melakukan langkah-langkah tambahan secara manual.\n",
        "\n",
        "Dengan demikian, **spaCy menawarkan metode yang lebih efisien dan terintegrasi** untuk NER dibandingkan dengan NLTK, yang memerlukan beberapa tahap pemrosesan terpisah."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3p4o06rUfJL"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# **Dependency Parsing**\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDu6hJHBUfJL"
      },
      "source": [
        "**Dependency Parsing**\n",
        "\n",
        "Dependency parsing adalah teknik yang digunakan untuk menganalisis struktur gramatikal sebuah kalimat. Teknik ini melibatkan identifikasi hubungan antar kata dalam kalimat dan pemahaman tentang bagaimana kata-kata tersebut saling terkait satu sama lain.\n",
        "\n",
        "**Komponen Utama**:\n",
        "\n",
        "1. **Struktur Gramatikal**, hal ini merujuk pada bagaimana kata-kata dalam sebuah kalimat saling berhubungan berdasarkan peran gramatikal mereka. Struktur ini biasanya diwakili dalam bentuk pohon di mana setiap kata adalah node, dan edges  merepresentasikan ketergantungan sintaktis.\n",
        "\n",
        "2. **Kata yang Terkait**: Dependency parsing menentukan bagaimana setiap kata bergantung pada kata lainnya. Misalnya, dalam kalimat \"Kucing itu duduk di atas tikar,\" kata \"duduk\" adalah kata kerja utama (root), dan ia memiliki ketergantungan seperti \"kucing\" (subjek) dan \"tikar\" (objek preposisi).\n",
        "\n",
        "3. **Jenis Hubungan**, Setiap hubungan ketergantungan memiliki jenis tertentu, seperti:\n",
        "   - **Subjek (nsubj)**: Kata benda atau kata ganti yang menjadi subjek dari kata kerja.\n",
        "   - **Objek (obj)**: Kata benda atau kata ganti yang menjadi objek dari kata kerja.\n",
        "   - **Modifikator (amod)**: Kata sifat atau kata keterangan yang memodifikasi kata lain.\n",
        "   - **Modifikator Preposisi (prep)**: Kata-kata yang menunjukkan hubungan antara kata benda dan bagian lain dari kalimat.\n",
        "\n",
        "**Contoh**:\n",
        "Untuk kalimat \"Kucing itu duduk di atas tikar,\" parsing ketergantungan mungkin menunjukkan:\n",
        "- **\"duduk\"** adalah root dari kalimat.\n",
        "- **\"Kucing itu\"** adalah subjek nominal (`nsubj`) dari \"duduk.\"\n",
        "- **\"di atas tikar\"** adalah frasa preposisional yang memodifikasi \"duduk\" (`prep`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr79dt1EUfJM"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "### **Percobaan Dependency Parsing dengan NLTK dan Spacy**\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWM7xCg7UfJM"
      },
      "source": [
        "#### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdEsLOxJUfJN",
        "outputId": "c0d52b78-5423-4757-ef70-4d1c94a64702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-16 10:24:24--  https://nlp.stanford.edu/software/stanford-corenlp-4.2.2.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.2.zip [following]\n",
            "--2024-09-16 10:24:24--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.2.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 504278711 (481M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-4.2.2.zip’\n",
            "\n",
            "stanford-corenlp-4. 100%[===================>] 480.92M  5.01MB/s    in 91s     \n",
            "\n",
            "2024-09-16 10:25:56 (5.28 MB/s) - ‘stanford-corenlp-4.2.2.zip’ saved [504278711/504278711]\n",
            "\n",
            "--2024-09-16 10:25:56--  https://nlp.stanford.edu/software/stanford-corenlp-4.2.2-models-english.jar\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.2-models-english.jar [following]\n",
            "--2024-09-16 10:25:56--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.2-models-english.jar\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444735210 (424M) [application/java-archive]\n",
            "Saving to: ‘stanford-corenlp-4.2.2-models-english.jar’\n",
            "\n",
            "stanford-corenlp-4. 100%[===================>] 424.13M  5.10MB/s    in 82s     \n",
            "\n",
            "2024-09-16 10:27:19 (5.14 MB/s) - ‘stanford-corenlp-4.2.2-models-english.jar’ saved [444735210/444735210]\n",
            "\n",
            "Archive:  /content/stanford-corenlp-4.2.2.zip\n",
            "   creating: stanford-corenlp-4.2.2/\n",
            "   creating: stanford-corenlp-4.2.2/patterns/\n",
            "  inflating: stanford-corenlp-4.2.2/patterns/names.txt  \n",
            " extracting: stanford-corenlp-4.2.2/patterns/otherpeople.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/patterns/example.properties  \n",
            "  inflating: stanford-corenlp-4.2.2/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/patterns/goldnames.txt  \n",
            " extracting: stanford-corenlp-4.2.2/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/patterns/stopwords.txt  \n",
            " extracting: stanford-corenlp-4.2.2/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/stanford-corenlp-4.2.2-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-4.2.2/input.txt.out  \n",
            "  inflating: stanford-corenlp-4.2.2/LIBRARY-LICENSES  \n",
            "  inflating: stanford-corenlp-4.2.2/istack-commons-runtime-3.0.7.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/build.xml  \n",
            "  inflating: stanford-corenlp-4.2.2/jollyday.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/StanfordDependenciesManual.pdf  \n",
            "  inflating: stanford-corenlp-4.2.2/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/pom.xml  \n",
            "  inflating: stanford-corenlp-4.2.2/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/LICENSE.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-4.2.2/joda-time-2.10.5-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/ejml-simple-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/ejml-ddense-0.39-sources.jar  \n",
            "   creating: stanford-corenlp-4.2.2/sutime/\n",
            "  inflating: stanford-corenlp-4.2.2/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/sutime/english.holidays.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/input.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/README.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/javax.json.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/jollyday-0.4.9-sources.jar  \n",
            "   creating: stanford-corenlp-4.2.2/tokensregex/\n",
            "  inflating: stanford-corenlp-4.2.2/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-4.2.2/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-4.2.2/ejml-core-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/joda-time.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/ejml-simple-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-4.2.2/xom.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/xom-1.3.2-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/corenlp.sh  \n",
            "  inflating: stanford-corenlp-4.2.2/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-4.2.2/input.txt.xml  \n",
            "  inflating: stanford-corenlp-4.2.2/protobuf-java-3.11.4.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/Makefile  \n",
            "  inflating: stanford-corenlp-4.2.2/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-4.2.2/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/stanford-corenlp-4.2.2-models.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/stanford-corenlp-4.2.2-javadoc.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/ejml-core-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-4.2.2/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/istack-commons-runtime-3.0.7-sources.jar  \n",
            "  inflating: stanford-corenlp-4.2.2/ejml-ddense-0.39.jar  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-1f20b97d0f67>:18: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
            "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
            "  parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Head            | Head POS   | Relation   | Dependent       | Dependent POS\n",
            "---------------------------------------------------------------------------\n",
            "charge          | VBP        | nsubj      | universities    | NNS       \n",
            "universities    | NNS        | compound   | Deemed          | NNP       \n",
            "charge          | VBP        | obj        | fees            | NNS       \n",
            "fees            | NNS        | amod       | huge            | JJ        \n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-4.2.2.zip\n",
        "\n",
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-4.2.2-models-english.jar\n",
        "\n",
        "!unzip /content/stanford-corenlp-4.2.2.zip\n",
        "\n",
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "\n",
        "# Path to CoreNLP jar unzipped\n",
        "jar_path = '/content/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar'\n",
        "\n",
        "# Path to CoreNLP model jar\n",
        "models_jar_path = '/content/stanford-corenlp-4.2.2-models-english.jar'\n",
        "\n",
        "sentence = 'Deemed universities charge huge fees'\n",
        "\n",
        "# Initialize StanfordDependency Parser from the path\n",
        "parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n",
        "\n",
        "# Parse the sentence\n",
        "result = parser.raw_parse(sentence)\n",
        "dependency = result.__next__()\n",
        "\n",
        "\n",
        "print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\".format('Head', 'Head POS','Relation','Dependent', 'Dependent POS'))\n",
        "print (\"-\" * 75)\n",
        "\n",
        "# Use dependency.triples() to extract the dependency triples in the form\n",
        "# ((head word, head POS), relation, (dependent word, dependent POS))\n",
        "for dep in list(dependency.triples()):\n",
        "  print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\"\n",
        "         .format(str(dep[0][0]),str(dep[0][1]), str(dep[1]), str(dep[2][0]),str(dep[2][1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f_ciTk4Y5nU"
      },
      "source": [
        "**Penjelasan Proses** :\n",
        "1. **Unduh dan Ekstrak Stanford CoreNLP**:\n",
        "   - Kode mengunduh dan mengekstrak Stanford CoreNLP dan model bahasa Inggris yang diperlukan untuk parsing teks.\n",
        "2. **Inisialisasi Parser**:\n",
        "   - Parser dependensi dari Stanford CoreNLP diinisialisasi dengan menentukan path ke file JAR yang berisi kode dan model bahasa.\n",
        "3. **Parse Kalimat**:\n",
        "   - Kalimat yang diberikan (\"Deemed universities charge huge fees\") diparse untuk mendapatkan struktur sintaksisnya dalam bentuk dependensi.\n",
        "4. **Tampilkan Hasil Parsing**:\n",
        "   - Hasil parsing ditampilkan dalam format tabel yang menunjukkan kata-kata dalam kalimat beserta hubungan dependensi mereka, seperti subjek, objek, dan modifikator.\n",
        "\n",
        "**Penjelasan Hasil** :\n",
        "\n",
        "komponen yang dihasilkan diantaranya :\n",
        "1. **Head**: Kata yang merupakan kata utama (head) dari hubungan dependensi.\n",
        "2. **Head POS**: Part-of-Speech tag dari kata yang merupakan head.\n",
        "3. **Relation**: Jenis hubungan dependensi antara head dan dependent.\n",
        "4. **Dependent**: Kata yang tergantung pada head dalam hubungan dependensi.\n",
        "5. **Dependent POS**: Part-of-Speech tag dari kata yang merupakan dependent.\n",
        "\n",
        "pembahasan hasil Dependency Parsing NLTK:\n",
        "1. **Baris 1**\n",
        "   - **Head**: `charge` (Verb, VBP)\n",
        "   - **Head POS**: VBP (Verb, non-3rd person singular present)\n",
        "   - **Relation**: `nsubj` (Nominal subject)\n",
        "   - **Dependent**: `universities` (Noun, NNS)\n",
        "   - **Dependent POS**: NNS (Noun, plural)\n",
        "   - **Penjelasan**: `universities` adalah subjek nominal (nsubj) dari `charge`. Dalam konteks ini, `charge` adalah kata kerja utama, dan `universities` adalah subjek yang melakukan tindakan `charge`.\n",
        "\n",
        "2. **Baris 2**\n",
        "   - **Head**: `universities` (Noun, NNS)\n",
        "   - **Head POS**: NNS (Noun, plural)\n",
        "   - **Relation**: `compound` (Compound noun)\n",
        "   - **Dependent**: `Deemed` (Proper noun, NNP)\n",
        "   - **Dependent POS**: NNP (Proper noun, singular)\n",
        "   - **Penjelasan**: `Deemed` adalah bagian dari noun phrase yang lebih besar yang dipimpin oleh `universities`, dan dalam hal ini, `Deemed` memberikan informasi tambahan mengenai `universities`.\n",
        "\n",
        "3. **Baris 3**\n",
        "   - **Head**: `charge` (Verb, VBP)\n",
        "   - **Head POS**: VBP (Verb, non-3rd person singular present)\n",
        "   - **Relation**: `obj` (Object)\n",
        "   - **Dependent**: `fees` (Noun, NNS)\n",
        "   - **Dependent POS**: NNS (Noun, plural)\n",
        "   - **Penjelasan**: `fees` adalah objek (obj) dari kata kerja `charge`. Ini menunjukkan bahwa `fees` adalah objek yang dikenakan oleh tindakan `charge`.\n",
        "\n",
        "4. **Baris 4**\n",
        "   - **Head**: `fees` (Noun, NNS)\n",
        "   - **Head POS**: NNS (Noun, plural)\n",
        "   - **Relation**: `amod` (Adjectival modifier)\n",
        "   - **Dependent**: `huge` (Adjective, JJ)\n",
        "   - **Dependent POS**: JJ (Adjective)\n",
        "   - **Penjelasan**: `huge` adalah modifikator adjectival (amod) dari `fees`. Ini memberikan informasi tambahan mengenai `fees`, menjelaskan bahwa `fees` tersebut besar.\n",
        "\n",
        "Secara keseluruhan, hasil dependency parsing ini menggambarkan bagaimana kata-kata dalam kalimat saling berhubungan, baik dalam konteks subjek, objek, maupun modifier. Parsing ini sangat berguna **untuk memahami struktur gramatikal dan semantik dari kalimat** dalam analisis teks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtUzlPvYZEiC"
      },
      "source": [
        "#### Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuRz1UVzUfJN",
        "outputId": "28724f88-54ff-4a1d-99b1-80e158d24023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deemed --> amod --> universities\n",
            "universities --> nsubj --> charge\n",
            "charge --> ROOT --> charge\n",
            "huge --> amod --> fees\n",
            "fees --> dobj --> charge\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load model bahasa Inggris dari Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Proses kalimat dengan NLP\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print dependency parsing\n",
        "for token in doc:\n",
        "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z3KBj7qZIky"
      },
      "source": [
        "**Penjelasan Kode:**\n",
        "1. **Import dan Load Model:**\n",
        "   - Kode mengimpor pustaka spaCy dan memuat model bahasa Inggris kecil (`en_core_web_sm`).\n",
        "2. **Proses Kalimat:**\n",
        "   - Kalimat yang diberikan diproses menggunakan model spaCy untuk menghasilkan objek yang berisi hasil analisis sintaksis.\n",
        "3. **Print Dependency Parsing:**\n",
        "   - Kode iterasi melalui setiap token dalam objek analisis dan mencetak hubungan dependensi antara token dan kata kepala mereka.\n",
        "\n",
        "**Penjelasan Hasil:**\n",
        "1. **Deemed --> amod --> universities**\n",
        "   - `Deemed` adalah **modifikator adjectival (amod)** dari `universities`.\n",
        "   - `universities` adalah kata kepala (head) dari `Deemed`.\n",
        "\n",
        "2. **universities --> nsubj --> charge**\n",
        "   - `universities` adalah **subjek nominal (nsubj)** dari `charge`.\n",
        "   - `charge` adalah kata kepala dari `universities`.\n",
        "\n",
        "3. **charge --> ROOT --> charge**\n",
        "   - `charge` adalah **kata kerja utama (ROOT)** dalam kalimat.\n",
        "   - `charge` adalah kata kepala dari dirinya sendiri dalam konteks ini.\n",
        "\n",
        "4. **huge --> amod --> fees**\n",
        "   - `huge` adalah **modifikator adjectival (amod)** dari `fees`.\n",
        "   - `fees` adalah kata kepala dari `huge`.\n",
        "\n",
        "5. **fees --> dobj --> charge**\n",
        "   - `fees` adalah **objek langsung (dobj)** dari `charge`.\n",
        "   - `charge` adalah kata kepala dari `fees`.\n",
        "\n",
        "**Kesimpulan:**\n",
        "Hasil parsing menunjukkan bagaimana kata-kata dalam kalimat saling berhubungan:\n",
        "- `Deemed` dan `huge` memberikan detail tambahan tentang `universities` dan `fees`.\n",
        "- `universities` adalah subjek dari `charge`, yang merupakan kata kerja utama.\n",
        "- `fees` adalah objek langsung dari `charge`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "#### **Kesimpulan**\n",
        "\n",
        "</div>\n",
        "\n",
        "**NLTK** dan **spaCy** melakukan dependency parsing, dengan pendekatan dan tingkat kemudahan yang berbeda. \n",
        "\n",
        "- **NLTK**:\n",
        "  - Menggunakan CoreNLP dari Stanford sebagai backend untuk dependency parsing, yang memerlukan konfigurasi server CoreNLP yang berjalan di `localhost`. \n",
        "  - Prosesnya memerlukan beberapa langkah: kita harus mengatur dan menjalankan server CoreNLP secara terpisah, dan kemudian menggunakan `CoreNLPDependencyParser` dari NLTK untuk melakukan parsing.\n",
        "  - Kelemahan utamanya adalah *ketergantungan pada server eksternal dan potensi masalah koneksi*.\n",
        "\n",
        "- **spaCy**:\n",
        "  - Menyediakan solusi yang lebih terintegrasi dan mudah digunakan dengan model bahasa yang sudah menyertakan kemampuan dependency parsing.\n",
        "  - Proses parsing dilakukan dalam satu langkah dengan metode `nlp()`, yang otomatis menangani tokenisasi, POS tagging, dan parsing ketergantungan.\n",
        "  - *Lebih cepat dan lebih mudah* digunakan karena tidak memerlukan konfigurasi server tambahan.\n",
        "\n",
        "Secara keseluruhan, spaCy menawarkan kemudahan dan efisiensi lebih besar dalam dependency parsing dibandingkan NLTK, yang memerlukan setup dan konfigurasi lebih rumit."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
